{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b65feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from torch_geometric) (2.11.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: requests in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from torch_geometric) (2.28.1)\n",
      "Requirement already satisfied: pyparsing in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: numpy in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from torch_geometric) (1.23.5)\n",
      "Requirement already satisfied: aiohttp in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.8.4)\n",
      "Requirement already satisfied: fsspec in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from torch_geometric) (2022.7.1)\n",
      "Requirement already satisfied: tqdm in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from torch_geometric) (4.64.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch_geometric) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (3.3)\n",
      "Installing collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d48fe682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n",
      "Epoch 0, Loss: 1.9531267881393433\n",
      "Epoch 10, Loss: 0.6011121273040771\n",
      "Epoch 20, Loss: 0.09413811564445496\n",
      "Epoch 30, Loss: 0.01961567811667919\n",
      "Epoch 40, Loss: 0.007385045289993286\n",
      "Epoch 50, Loss: 0.004220667760819197\n",
      "Epoch 60, Loss: 0.0030528465285897255\n",
      "Epoch 70, Loss: 0.0024841390550136566\n",
      "Epoch 80, Loss: 0.002137193689122796\n",
      "Epoch 90, Loss: 0.0018891923828050494\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "%time\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b77b0181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc18442",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "GCN aggregates features from a node’s neighbors using graph convolutions. This allows the network to learn representations based on both node features and graph structure.\n",
    "The Cora dataset is used to classify nodes into one of 7 research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb882b",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "\n",
    "1. What would happen if we added more GCN layers (e.g., 3 layers instead of 2)? How would this affect over-smoothing?\n",
    "**Ans:** As we can see below, adding more GCN layers resulted in less training loss. However, if we continue to add more GCN layers in an effort to improve the model's awareness of the graph structure through message passing, we might end up creating a model that treats all nodes the same. In other words, the node feature representations would converge to indistinguishable vectors (known as over-smoothing). This is because as the model passes through more layers, the size of the computational graphs increases which increases the probability of accessing the same nodes. *(See code below)*\n",
    "\n",
    "\n",
    "2. What would happen if we used a larger hidden dimension (e.g., 64 instead of 16)? How would this impact the model's capacity?\n",
    "**Ans:** Increasing the size of the hidden dimension led to a training loss of 0. This could mean that the model overfit to the training data and won't generalize as well to test data. It is better if we keep a smaller hidden dimension in order to prevent the GCN from overfitting. *(See code below)*\n",
    "\n",
    "\n",
    "3. What would happen if we replaced ReLU activation with a sigmoid function? Would the performance change?\n",
    "**Ans:** Replacing the ReLU with sigmoid led to a training loss of nearly 0 for every epoch. This is likely a cause of the vanishing gradients problem that arises when using the sigmoid activation function. We should stick with ReLU for this use case because the gradients are either 1 (for positive inputs) or 0 (for negative inputs) so gradients don't shrink as drastically. *(See code below)*\n",
    "\n",
    "\n",
    "4. What would happen if we trained on only 10% of the nodes and tested on the remaining 90%? How would the performance be affected?\n",
    "**Ans:** If we trained only on 10% of nodes, then the model's performance would worsen because it hasn't seen the rest of the graph data. In other words, performance relies heavily on the model's ability to learn graph structure and node feature representation during training. By removing most of the training data, the model's capability to learn the structure of the data would erode thus leading to a poor test performance.\n",
    "\n",
    "\n",
    "5. What would happen if we used a different optimizer (e.g., RMSprop) instead of Adam? Would it affect the convergence speed?\n",
    "**Ans:** Using a different optimizer would affect convergence speed since each optimizer has its own way of adjusting step size and finding local minimas. In the case below where we replaced Adam with RMSprop, it converged slightly faster (1µs faster) compared to the base model above.\n",
    "\n",
    "Extra credit: \n",
    "1. What would happen if we used edge weights (non-binary) in the adjacency matrix? How would it affect message passing?\n",
    "**Ans:** Using edge weights in the adjacency matrix should give the model more information about the structure of the graph and the relative positions/semantic relationships between nodes. Theoretically, this would benefit message passing by allowing the model to recognize the strength of relationships between nodes.\n",
    "\n",
    "\n",
    "2. What would happen if we removed the log-softmax function in the output layer? Would the loss function still work correctly?\n",
    "**Ans:** My initial hypothesis is that removing the log-softmax function wouldn't allow the loss function to work correctly since the output of the network would be incorrectly formatted. However, after trying this below (see code), it seems to work based off the training loss (which consistently reduces over each epoch and is similar loss to the above model).\n",
    "\n",
    "## No points, just for you to think about:\n",
    "1. What would happen if we applied dropout to the node features during training? How would it affect the model’s generalization?\n",
    "2. What would happen if we used mean-pooling instead of summing the messages in the GCN layers?\n",
    "3. What would happen if we pre-trained the node features using a different algorithm, like Node2Vec, before feeding them into the GCN?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2da383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "265ef3c8",
   "metadata": {},
   "source": [
    "#### Question 1 - Adding 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11b1f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0015451112994924188\n",
      "Epoch 10, Loss: 4.580696622724645e-05\n",
      "Epoch 20, Loss: 8.037974112085067e-06\n",
      "Epoch 30, Loss: 3.624783630584716e-06\n",
      "Epoch 40, Loss: 2.468470484018326e-06\n",
      "Epoch 50, Loss: 2.0367670003906824e-06\n",
      "Epoch 60, Loss: 1.8315585066375206e-06\n",
      "Epoch 70, Loss: 1.6910628346522572e-06\n",
      "Epoch 80, Loss: 1.5829236872377805e-06\n",
      "Epoch 90, Loss: 1.4867054005662794e-06\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define a 3-layer GCN\n",
    "class GCN3(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN3, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 250)\n",
    "        self.conv2 = GCNConv(250, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model_3 = GCN3(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model_3.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb08bd3",
   "metadata": {},
   "source": [
    "#### Question 2 - Increasing size of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ad4c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.4109230050962651e-06\n",
      "Epoch 10, Loss: 8.685246655204537e-08\n",
      "Epoch 20, Loss: 1.362391799375473e-08\n",
      "Epoch 30, Loss: 5.108969247658024e-09\n",
      "Epoch 40, Loss: 2.554484623829012e-09\n",
      "Epoch 50, Loss: 1.7029897492193413e-09\n",
      "Epoch 60, Loss: 8.514948746096707e-10\n",
      "Epoch 70, Loss: 8.514948746096707e-10\n",
      "Epoch 80, Loss: 0.0\n",
      "Epoch 90, Loss: 0.0\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model_hidden = GCN(input_dim=dataset.num_node_features, hidden_dim=64, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model_hidden.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b10d9",
   "metadata": {},
   "source": [
    "#### Question 3 - Replace ReLU with Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abfaa7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3126474388845963e-06\n",
      "Epoch 10, Loss: 1.4730856889855204e-07\n",
      "Epoch 20, Loss: 2.7247834211152622e-08\n",
      "Epoch 30, Loss: 1.1069433369925719e-08\n",
      "Epoch 40, Loss: 6.811958996877365e-09\n",
      "Epoch 50, Loss: 5.960464122267695e-09\n",
      "Epoch 60, Loss: 3.4059794984386826e-09\n",
      "Epoch 70, Loss: 3.4059794984386826e-09\n",
      "Epoch 80, Loss: 1.7029897492193413e-09\n",
      "Epoch 90, Loss: 8.514948746096707e-10\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model_hidden = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model_hidden.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc7908e",
   "metadata": {},
   "source": [
    "#### Question 5 - Replace Adam with RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "314b1ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 6.2 µs\n",
      "Epoch 0, Loss: 0.001809474197216332\n",
      "Epoch 10, Loss: 0.0005646863719448447\n",
      "Epoch 20, Loss: 0.00028500787448138\n",
      "Epoch 30, Loss: 0.00019343489839229733\n",
      "Epoch 40, Loss: 0.00014670485688839108\n",
      "Epoch 50, Loss: 0.00011805366375483572\n",
      "Epoch 60, Loss: 9.856133692665026e-05\n",
      "Epoch 70, Loss: 8.438501390628517e-05\n",
      "Epoch 80, Loss: 7.36061847419478e-05\n",
      "Epoch 90, Loss: 6.512028630822897e-05\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model_hidden = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model_hidden.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12379c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0016936450265347958\n",
      "Epoch 10, Loss: 5.1432183681754395e-05\n",
      "Epoch 20, Loss: 9.090367711905856e-06\n",
      "Epoch 30, Loss: 4.063294454681454e-06\n",
      "Epoch 40, Loss: 2.7733008209906984e-06\n",
      "Epoch 50, Loss: 2.2777373942517443e-06\n",
      "Epoch 60, Loss: 2.0265488274162635e-06\n",
      "Epoch 70, Loss: 1.8690238903218415e-06\n",
      "Epoch 80, Loss: 1.7413008208677638e-06\n",
      "Epoch 90, Loss: 1.6382705325668212e-06\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define a 2-layer GCN\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model2 = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model2.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0354e26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea4102f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
