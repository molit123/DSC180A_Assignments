{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f016489f",
   "metadata": {},
   "source": [
    "\n",
    "### Objective: \n",
    "\n",
    "In this assignment, implement the Node2Vec algorithm, a random-walk-based GNN, to learn node embeddings. Train a classifier using the learned embeddings to predict node labels.\n",
    "\n",
    "### Dataset: \n",
    "\n",
    "Cora dataset: The dataset consists of 2,708 nodes (scientific publications) with 5,429 edges (citations between publications). Each node has a feature vector of size 1,433, and there are 7 classes (research topics).\n",
    "Skeleton Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc12528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting node2vec\n",
      "  Downloading node2vec-0.5.0-py3-none-any.whl (7.2 kB)\n",
      "Collecting numpy<2.0.0,>=1.24.0\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gensim<5.0.0,>=4.3.0\n",
      "  Downloading gensim-4.3.3-cp39-cp39-macosx_10_9_x86_64.whl (24.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting networkx<4.0.0,>=3.1.0\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.66.1\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib<2.0.0,>=1.4.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/mohit/opt/anaconda3/lib/python3.9/site-packages (from gensim<5.0.0,>=4.3.0->node2vec) (5.2.1)\n",
      "Collecting numpy<2.0.0,>=1.24.0\n",
      "  Downloading numpy-1.24.4-cp39-cp39-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: tqdm, numpy, networkx, joblib, gensim, node2vec\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.8.4\n",
      "    Uninstalling networkx-2.8.4:\n",
      "      Successfully uninstalled networkx-2.8.4\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.1.2\n",
      "    Uninstalling gensim-4.1.2:\n",
      "      Successfully uninstalled gensim-4.1.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.4 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.4 which is incompatible.\n",
      "argilla 1.7.0 requires numpy<1.24.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.3.3 joblib-1.4.2 networkx-3.2.1 node2vec-0.5.0 numpy-1.24.4 tqdm-4.66.5\n"
     ]
    }
   ],
   "source": [
    "!pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c492a4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601811a2f6734f039a6e7e21c63d0d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 100/100 [00:27<00:00,  3.60it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 100/100 [00:29<00:00,  3.40it/s]\n",
      "/var/folders/xy/dc9cct710_3fw4sk4kwnptnm0000gn/T/ipykernel_6643/523172613.py:45: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.031139850616455\n",
      "Epoch 10, Loss: 1.2861847877502441\n",
      "Epoch 20, Loss: 0.9358958601951599\n",
      "Epoch 30, Loss: 0.7734335064888\n",
      "Epoch 40, Loss: 0.6942480206489563\n",
      "Epoch 50, Loss: 0.649330198764801\n",
      "Epoch 60, Loss: 0.6194333434104919\n",
      "Epoch 70, Loss: 0.5978078842163086\n",
      "Epoch 80, Loss: 0.581070065498352\n",
      "Epoch 90, Loss: 0.5675488114356995\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "from node2vec import Node2Vec  # Importing Node2Vec for the random walk\n",
    "\n",
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "\n",
    "# Prepare data\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert to networkx for random walk\n",
    "import networkx as nx\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ee022",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "Node2Vec generates node embeddings by simulating random walks on the graph. These walks capture structural properties of nodes.\n",
    "The generated embeddings are then used to train a classifier for predicting node labels.\n",
    "The Cora dataset is a benchmark graph where nodes are papers and edges are citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b3004",
   "metadata": {},
   "source": [
    "## Questions (1 point each):\n",
    "1. What would happen if we increased the number of walks (num_walks) per node? How might this affect the learned embeddings?\n",
    "**Ans:** Increasing the number of walks per node will increase the amount of information gathered about each node’s neighborhood. As a result, the embeddings can better capture both the local and global structure of the graph, potentially improving their quality. The learned embeddings might become more stable and consistent, leading to better generalization. However, this increases the computational cost of running the classifier and could potentially lead to overfitting. (See code below).\n",
    "\n",
    "\n",
    "\n",
    "2. What would happen if we reduced the walk length (walk_length)? How would this influence the structural information captured by the embeddings?\n",
    "**Ans:** Shorter walks lead to more localized information. As a result, the embeddings will reflect local structural properties but may not capture overall structure or global relationships within the graph. Reducing the walk length decreases the number of steps taken in each random walk, which will reduce the computational time needed to generate the walks and train the embeddings. However, this may lead to poorer performance especially for tasks that require a more global understanding of the structure of the data.\n",
    "\n",
    "\n",
    "3. What would happen if we used directed edges instead of undirected edges for the random walks?\n",
    "**Ans:** Directed edges reflect the asymmetric relationships between nodes (e.g., in a citation network, paper A cites paper B, but B may not cite A). This allows the embeddings to capture the directionality of these relationships, leading to more detailed representations that account for the flow of information or relationships in the graph. Additionally, since directed edges only allow walks in certain directions, this might change the nieghbors encountered thus leading to differing embeddings. For example, if paper A cites paper B but not the other way around, A and B will have distinct representations as opposed to an undirected edge where they will have more similar representations. Random walks might also get trapped in certain areas of the graph because of directionality.\n",
    "\n",
    "\n",
    "4. What would happen if we added more features to the nodes (e.g., 2000-dimensional features instead of 1433)?\n",
    "**Ans:** Adding more features increases the potential for richer and more expressive node embeddings since each node's representation can contain more information. This can also lead to improved performance since the model has more input data to capture complex semantic relationships. However, this also leads to increases memory usage and computational cost, especially for large graphs, since each node contains more information. Moreover, the additional features might be redundant and cause overfitting.\n",
    "\n",
    "\n",
    "5. What would happen if we used a different dataset with more classes? Would the classifier performance change significantly?\n",
    "**Ans:** If we used a different dataset with more classes, it would become harder to distinguish between various classes for each node. The classifier will need to learn more complex decision boundaries potentially leading to lower overall accuracy. This also creates the need for more expressive embeddings to distinguish various node classes. If the new dataset has imbalanced class distributions, the classifier might struggle with performance on the less frequent classes. The risk of overfitting also increases, introducing the need for some type of regularization (L1, L2, or dropout).\n",
    "\n",
    "\n",
    "6. What would happen if we used a larger embedding dimension (e.g., 128 instead of 64)? How would this affect the model’s performance and training time?\n",
    "**Ans:** A larger embedding dimension gives the model more representational capacity. This means that the embeddings can capture more detailed structural information and node attributes, allowing for more expressive node representations. In some cases, this can also lead to improved performance because the model has more information to distinguish between various node classes. For example, if two nodes are structurally similar but belong to different classes, increasing the embedding dimension might allow the classifier to absorb more information about the node's intricate differences. However, it can also lead to the curse of dimensionality which makes it harder for the model to generalize and find meaningful patterns in the data. It will also lead to increased training time and memory usage because (in this case) we are doubling the size of each node's embedding vector. (See code below).\n",
    "\n",
    "\n",
    "\n",
    "### Extra credit: \n",
    "1. What would happen if we increased the window size (window) for the skip-gram model? How would it affect the embedding quality?\n",
    "**Ans:** A wider window size allows the model to consider a broader neighborhood around each node during training. Embeddings will incorporate information from nodes that are farther away, which can help capture more global structural properties of the graph. By considering a larger context, the embeddings can generalize better across the graph, capturing patterns that span beyond immediate neighbors. However, there is a risk that the embeddings for all nodes in a graph will become overly similar, especially in densely connected regions, leading to over-smoothing and poorer localized performance.\n",
    "\n",
    "\n",
    "## No points, just for you to think about\n",
    "7. What would happen if we removed self-loops from the graph before training Node2Vec?\n",
    "\n",
    "9. What would happen if we applied normalization to the node embeddings before feeding them to the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca476c8d",
   "metadata": {},
   "source": [
    "#### Question 1 - Increasing num_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1353dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a5ac2e4a5e46a9aef8929e0a318d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 250/250 [01:05<00:00,  3.80it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 250/250 [01:08<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.0590436458587646\n",
      "Epoch 10, Loss: 1.3194230794906616\n",
      "Epoch 20, Loss: 0.9471554756164551\n",
      "Epoch 30, Loss: 0.7831547260284424\n",
      "Epoch 40, Loss: 0.702115535736084\n",
      "Epoch 50, Loss: 0.6545844078063965\n",
      "Epoch 60, Loss: 0.6221349835395813\n",
      "Epoch 70, Loss: 0.5981424450874329\n",
      "Epoch 80, Loss: 0.5794676542282104\n",
      "Epoch 90, Loss: 0.5643690824508667\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=500, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(64, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d73d2",
   "metadata": {},
   "source": [
    "#### Question 6 - Increased embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a2586a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93193e345334fd795209e39935315b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 2): 100%|██████████| 250/250 [01:07<00:00,  3.73it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 250/250 [01:07<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.025282382965088\n",
      "Epoch 10, Loss: 1.1157904863357544\n",
      "Epoch 20, Loss: 0.7768505811691284\n",
      "Epoch 30, Loss: 0.6613138914108276\n",
      "Epoch 40, Loss: 0.6017512679100037\n",
      "Epoch 50, Loss: 0.5619878768920898\n",
      "Epoch 60, Loss: 0.532753586769104\n",
      "Epoch 70, Loss: 0.5096790790557861\n",
      "Epoch 80, Loss: 0.4906686544418335\n",
      "Epoch 90, Loss: 0.4746033251285553\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Node2Vec configuration\n",
    "node2vec = Node2Vec(G, dimensions=128, walk_length=30, num_walks=500, workers=2) \n",
    "model = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# Embeddings for each node\n",
    "embeddings = model.wv  # Node embeddings\n",
    "\n",
    "# Define a simple classifier\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize classifier and optimizer\n",
    "classifier = Classifier(128, 7)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get node embeddings as input\n",
    "    output = classifier(torch.tensor([embeddings[str(i)] for i in range(data.num_nodes)]))\n",
    "    \n",
    "    loss = criterion(output, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
